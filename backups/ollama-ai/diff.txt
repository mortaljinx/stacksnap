--- previous
+++ current
@@ -1,54 +1,6 @@
-version: "3.8"- services:--  # =========================================================-  # OLLAMA (local AI engine - internal only)-  # =========================================================+  openwebui:+    image: ghcr.io/open-webui/open-webui:main   ollama:     image: ollama/ollama:latest-    container_name: ollama-    restart: unless-stopped-    volumes:-      - /mnt/nvme/docker/ollama:/root/.ollama-    environment:-      - OLLAMA_NUM_THREADS=8-      - OLLAMA_MAX_LOADED_MODELS=1-      - OLLAMA_NUM_PARALLEL=1-      - OLLAMA_KEEP_ALIVE=24h-    networks:-      - ichor-    # ‚ùå no ports needed unless you want LAN access-    # remove 11434:11434 for better security --  # =========================================================-  # OPEN WEB UI (chat frontend for Ollama)-  # =========================================================-  openwebui:-    image: ghcr.io/open-webui/open-webui:main-    container_name: openwebui-    restart: unless-stopped-    depends_on:-      - ollama-    volumes:-      - /mnt/nvme/docker/openwebui:/app/backend/data-    environment:-      - OLLAMA_BASE_URL=http://ollama:11434-    networks:-      - ichor--    labels:-      - "traefik.enable=true"-      - "traefik.http.routers.openwebui.rule=Host(`ai.mortalsrv.uk`)"-      - "traefik.http.routers.openwebui.entrypoints=websecure"-      - "traefik.http.routers.openwebui.tls=true"-      - "traefik.http.services.openwebui.loadbalancer.server.port=8080"---# =========================================================-# NETWORKS-# =========================================================-networks:-  ichor:-    external: true